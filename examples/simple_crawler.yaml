# Simple web crawler workflow example
# This workflow crawls a website, discovers links, and extracts basic data

start_urls:
  - "https://example.com"

max_depth: 2
max_pages: 100
rate_limit_delay: 1000

# URL Discovery: Find and queue new URLs to crawl
url_discovery:
  - id: "extract_links"
    type: "extract_links"
    name: "Extract all links from page"
    params:
      selector: "a[href]"
    output_key: "discovered_links"

  - id: "filter_urls"
    type: "filter_urls"
    name: "Filter URLs to same domain"
    params:
      same_domain: true
      exclude_patterns:
        - ".*\\.pdf$"
        - ".*\\.zip$"
        - ".*logout.*"
    dependencies:
      - "extract_links"

# Data Extraction: Extract structured data from each page
data_extraction:
  - id: "extract_title"
    type: "extract"
    name: "Extract page title"
    params:
      selector: "h1"
      type: "text"
      transform:
        - type: "trim"
    output_key: "title"

  - id: "extract_description"
    type: "extract"
    name: "Extract meta description"
    params:
      selector: "meta[name='description']"
      type: "attr"
      attribute: "content"
      default_value: ""
    output_key: "description"

  - id: "extract_content"
    type: "extract"
    name: "Extract main content"
    params:
      selector: "article, main, .content"
      type: "text"
      transform:
        - type: "trim"
    output_key: "content"
    optional: true

# Storage configuration
storage:
  type: "database"
  table_name: "crawled_pages"
